num_vec = c(1, 2, 3, 5, 99)
class(num_vec)
num_vec = c(1, 3, 5,99)
class(num_vec)
install.pakages("MASSS")
library(MASS)
install.packages("MASSS")
install.packages("MASS")
install.packages("MASS")
install.packages("MASS")
library(MASS)
attach(Boston)
?Boston
head(Boston)
head(Boston)
dim(Boston)
names(Boston)
str(Boston)
nrow(Boston)
ncol(Boston)
summary(Boston)
statistics
summary(Boston$crim)
library(ISLR)
library(MASS)
library(boot)
set.seed(1)
install.packages(ISLR)
install.packages("ISLR")
library(ISLR)
library(MASS)
library(boot)
set.seed(1)
??cv.glm
help("sample")
train = sample(392, 196)
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
attach(Auto)
mean((mpg-predict(lm.fit, Auto))[-train]^2)
lm.fit2 <- lm(mpg~poly(horsepower, 2), data = Auto, subset = train)
mean((mpg-predict(lm.fit2, Auto))[-train]^2)
mean((mpg-predict(lm.fit3, Auto))[-train]^2)
# Cubic Regression Line
lm.fit3 <- lm(mpg~poly(horsepower, 3), data = Auto, subset = train)
mean((mpg-predict(lm.fit3, Auto))[-train]^2)
library(ISLR)
library(MASS)
library(boot)
set.seed(1)
train = sample(392, 196)
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
attach(Auto)
mean((mpg-predict(lm.fit, Auto))[-train]^2)
# Quadratic Regression Line
lm.fit2 <- lm(mpg~poly(horsepower, 2), data = Auto, subset = train)
mean((mpg-predict(lm.fit2, Auto))[-train]^2)
# Cubic Regression Line
lm.fit3 <- lm(mpg~poly(horsepower, 3), data = Auto, subset = train)
mean((mpg-predict(lm.fit3, Auto))[-train]^2)
set.seed(2)
set.seed(2)
train = sample(392,196)
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit2 <- lm(mpg~poly(horsepower,2), data = Auto, subset = train) # Quadratic
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
set.seed(2)
train = sample(392,196)
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2 <- lm(mpg~poly(horsepower,2), data = Auto, subset = train) # Quadratic
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3 <- lm(mpg~poly(horsepower,3), data = Auto, subset = train) # Cubic
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
set.seed(17)
cv.error.10 = rep(0,10)
for(i in 1:10){
glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error.10[i] = cv.glm(Auto,glm.fit, K=10) $delta[1]
}
cv.error.10
library(ISLR)
library(MASS)
library(boot)
set.seed(17)
help('rep')
cv.error.10 = rep(0,10)
for(i in 1:10){
glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error.10[i] = cv.glm(Auto,glm.fit, K=10) $delta[1]
}
cv.error.10
wine_data <- read.table("http://archive.ics.uci.edu/ml/machinelearning-databases/wine/wine.data", sep = ",")
wine_data <- read.table("https://archive.ics.uci.edu/dataset/109/wine", sep = ",")
head(wine_data)
wine_data <- read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep = ",")
head(wine_data)
colnames(wine_data) <- c("Cvs", "Alcohol",
"Malic_Acid", "Ash", "Alkalinity_of_Ash",
"Magnesium", "Total_Phenols", "Flavanoids", "NonFlavanoid_Phenols",
"Proanthocyanins", "Color_Intensity", "Hue", "OD280/OD315_of_Diluted_Wine",
"Proline")
head(wine_data)
heatmap(cor(wine_data), Rowv = NA, Colv = NA)
summary(wine_data)
cultivar_classes <- factor(wine_data$Cvs)
cultivar_classes
wine_data_pca = prcomp(scale(wine_data[,-1]))
summary(wine_data_pca)
glm.fit <- glm(mpg~horsepower, data=Auto) # we have not used the "family" argument here.
setwd("C:/Users/ghpan/Documents/DataAnalytics2023_Tony_Min/Labs/lab8")
library(cvTools)
library(robustbase)
data(coleman)
call <- call("lmrob", formula = Y ~ .)
cal
call
# set up folds for cross-validation
folds <- cvFolds(nrow(coleman), K = 5, R = 10)
# perform cross-validation
cvTool(call, data = coleman, y = coleman$Y, cost = rtmspe, folds = folds, costArgs = list(trim = 0.1))
#vary K and R
#look at cvfits, use densityplot,
tuning <- list(tuning.psi=seq(2., 6., 20))
cvFitsLmrob <- cvTuning(call, data = coleman, y = coleman$Y, tuning = tuning, cost = rtmspe, folds = folds, costArgs = list(trim = 0.1))
# look at output
cvFitsLmrob
# summarize
aggregate(cvFitsLmrob, summary)
coleman
# look at output
cvFitsLmrob
# summarize
aggregate(cvFitsLmrob, summary)
library(MASS)
data(mammals)
mammals.glm <- glm(log(brain) ~ log(body), data = mammals)
(cv.err <- cv.glm(mammals, mammals.glm)$delta)
mammals.glm <- glm(log(brain) ~ log(body), data = mammals)
(cv.err <- cv.glm(mammals, mammals.glm)$delta)
library(boot)
(cv.err <- cv.glm(mammals, mammals.glm)$delta)
(cv.err.6 <- cv.glm(mammals, mammals.glm, K = 6)$delta)
# Leave-one-out cross-validation estimate without any extra model-fitting.
muhat <- fitted(mammals.glm)
mammals.diag <- glm.diag(mammals.glm)
(cv.esterr <- mean((mammals.glm$y - muhat)^2/(1 - mammals.diag$h)^2))
# leave-one-out and 11-fold cross-validation prediction error for
# the nodal data set.  Since the response is a binary variable
# an appropriate cost function is
library(boot)
data(nodal)
cost <- function(r, pi = 0) mean(abs(r-pi) > 0.5)
nodal.glm <- glm(r ~ stage+xray+acid, binomial, data = nodal)
(cv.err <- cv.glm(nodal, nodal.glm, cost, K = nrow(nodal))$delta)
(cv.11.err <- cv.glm(nodal, nodal.glm, cost, K = 11)$delta)
install.packages("robustbase")
install.packages("robustbase")
install.packages("robustbase")
install.packages("cvTools")
library("robustbase")
require(cvTools)
data("coleman")
set.seed(1234) # set seed for reproducibility
## set up folds for cross-validation
folds <- cvFolds(nrow(coleman), K = 5, R = 10)
## compare raw and reweighted LTS estimators for
## 50% and 75% subsets
# 50% subsets
fitLts50 <- ltsReg(Y ~ ., data = coleman, alpha = 0.5)
cvFitLts50 <- cvLts(fitLts50, cost = rtmspe, folds = folds,
fit = "both", trim = 0.1)
# 75% subsets
fitLts75 <- ltsReg(Y ~ ., data = coleman, alpha = 0.75)
cvFitLts75 <- cvLts(fitLts75, cost = rtmspe, folds = folds,
fit = "both", trim = 0.1)
# combine results into one object
cvFitsLts <- cvSelect("0.5" = cvFitLts50, "0.75" = cvFitLts75)
cvFitsLts
# "cv" object
ncv(cvFitLts50)
nfits(cvFitLts50)
cvNames(cvFitLts50)
cvNames(cvFitLts50) <- c("improved", "initial")
fits(cvFitLts50)
cvFitLts50
# "cvSelect" object
ncv(cvFitsLts)
nfits(cvFitsLts)
cvNames(cvFitsLts)
cvNames(cvFitsLts) <- c("improved", "initial")
fits(cvFitsLts)
fits(cvFitsLts) <- 1:2
cvFitsLts
